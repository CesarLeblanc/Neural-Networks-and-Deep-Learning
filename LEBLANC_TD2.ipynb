{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LEBLANC_TD2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPd5Ugu9qglSJnQwX5GiPck"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qFmwscWx_akC"},"source":["# Exercise 1"]},{"cell_type":"code","metadata":{"id":"Y3noxSTn94xz"},"source":["from random import random\r\n","\r\n","# Initialize a network\r\n","def initialize_network(n_inputs, n_hidden, n_outputs):\r\n","\tnetwork = list()\r\n","\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\r\n","\tnetwork.append(hidden_layer)\r\n","\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\r\n","\tnetwork.append(output_layer)\r\n","\treturn network"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6CuJFJ-YodM0"},"source":["For the hidden layer we create n_hidden neurons and each neuron in the hidden layer has n_inputs + 1 weights, one for each input column in a dataset and an additional one for the bias.\n","\n","The output layer that connects to the hidden layer has n_outputs neurons, each with n_hidden + 1 weights. This means that each neuron in the output layer connects to (has a weight for) each neuron in the hidden layer."]},{"cell_type":"markdown","metadata":{"id":"_r5inKZg_8lw"},"source":["# Exercise 2"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cRHISZ0nACE-","executionInfo":{"status":"ok","timestamp":1612945794184,"user_tz":-60,"elapsed":557,"user":{"displayName":"César Leblanc","photoUrl":"","userId":"05479109431525987865"}},"outputId":"b6e45ae6-3da5-4408-ba12-ea76e73962fa"},"source":["from random import seed\r\n","\r\n","seed(1)\r\n","network = initialize_network(2, 1, 2)\r\n","for layer in network:\r\n","\tprint(layer)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n","[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pHbO49vJozFH"},"source":["The hidden layer has one neuron with 2 input weights plus the bias and the output layer has 2 neurons, each with 1 weight plus the bias."]},{"cell_type":"markdown","metadata":{"id":"01x2Jg-wAscs"},"source":["# Exercise 3"]},{"cell_type":"code","metadata":{"id":"KBuejbrRAud6"},"source":["# Calculate neuron activation for an input\r\n","def activate(weights, inputs):\r\n","\tactivation = weights[-1]\r\n","\tfor i in range(len(weights)-1):\r\n","\t\tactivation += weights[i] * inputs[i]\r\n","\treturn activation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wKkF8-dFph9Z"},"source":["Neuron activation is calculated as the weighted sum of the inputs : activation = sum(weight_i * input_i) + bias\n","\n","The function assumes that the bias is the last weight in the list of weights."]},{"cell_type":"markdown","metadata":{"id":"tHaVMm8kBA4X"},"source":["# Exercise 4"]},{"cell_type":"code","metadata":{"id":"OH6WJseDBEHT"},"source":["from math import exp\r\n","\r\n","# Transfer neuron activation\r\n","def transfer(activation):\r\n","\treturn 1.0 / (1.0 + exp(-activation))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9e1EWRXUqFyu"},"source":["The sigmoid activation function looks like an S shape, it’s also called the logistic function. It can take any input value and produce a number between 0 and 1 on an S-curve. It is also a function of which we can easily calculate the derivative (slope) that we will need later when backpropagating error."]},{"cell_type":"markdown","metadata":{"id":"2FMBa7UoBIjd"},"source":["# Exercise 5"]},{"cell_type":"code","metadata":{"id":"KchCyWrVBKKB"},"source":["# Forward propagate input to a network output\r\n","def forward_propagate(network, row):\r\n","\tinputs = row\r\n","\tfor layer in network:\r\n","\t\tnew_inputs = []\r\n","\t\tfor neuron in layer:\r\n","\t\t\tactivation = activate(neuron['weights'], inputs)\r\n","\t\t\tneuron['output'] = transfer(activation)\r\n","\t\t\tnew_inputs.append(neuron['output'])\r\n","\t\tinputs = new_inputs\r\n","\treturn inputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0xj_cC4sqtvX"},"source":["A neuron’s output value is stored in the neuron with the name 'output'. We collect the outputs for a layer in an array named new_inputs that becomes the array inputs and is used as inputs for the following layer."]},{"cell_type":"markdown","metadata":{"id":"pN1saWr0ByoI"},"source":["# Exercise 6"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kN1W90NjB0J2","executionInfo":{"status":"ok","timestamp":1612945822705,"user_tz":-60,"elapsed":506,"user":{"displayName":"César Leblanc","photoUrl":"","userId":"05479109431525987865"}},"outputId":"75abd3fd-4d98-41b8-9bdd-88c10da5f382"},"source":["seed(1)\r\n","\r\n","# test forward propagation\r\n","network = initialize_network(2, 1, 2)\r\n","row = [0, 1]\r\n","output = forward_propagate(network, row)\r\n","print(output)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.6699713080575971, 0.7361939293022448]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UdseTIyPrdE-"},"source":["We define our network inline with one hidden neuron that expects 2 input values and an output layer with two neurons.\n","\n","Running the example propagates the input pattern [0, 1] and produces an output value that is printed. Because the output layer has two neurons, we get a list of two numbers as output."]},{"cell_type":"markdown","metadata":{"id":"nM8MpFT5Ciif"},"source":["# Exercise 7"]},{"cell_type":"code","metadata":{"id":"nvpBcJkrCkEC"},"source":["# Calculate the derivative of an neuron output\r\n","def transfer_derivative(output):\r\n","\treturn output * (1.0 - output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-vpGJXuVtkiX"},"source":["We are using the sigmoid transfer function, the derivative of which can be calculated as follows: derivative = output * (1.0 - output)"]},{"cell_type":"markdown","metadata":{"id":"h8Su9rKhD_kT"},"source":["# Exercise 8"]},{"cell_type":"code","metadata":{"id":"P1FVKeNAEBX8"},"source":["# Backpropagate error and store in neurons\r\n","def backward_propagate_error(network, expected):\r\n","\tfor i in reversed(range(len(network))):\r\n","\t\tlayer = network[i]\r\n","\t\terrors = list()\r\n","\t\tif i != len(network)-1:\r\n","\t\t\tfor j in range(len(layer)):\r\n","\t\t\t\terror = 0.0\r\n","\t\t\t\tfor neuron in network[i + 1]:\r\n","\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\r\n","\t\t\t\terrors.append(error)\r\n","\t\telse:\r\n","\t\t\tfor j in range(len(layer)):\r\n","\t\t\t\tneuron = layer[j]\r\n","\t\t\t\terrors.append(expected[j] - neuron['output'])\r\n","\t\tfor j in range(len(layer)):\r\n","\t\t\tneuron = layer[j]\r\n","\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ijVWZ47u7Sp"},"source":["The error for a given output neuron can be calculated as follows: error = (expected - output) * transfer_derivative(output)\n","\n","The error for a given hidden neuron can be calculated as follows: error = (weight_k * error_j) * transfer_derivative(output)\n","\n","The error signal calculated for each neuron is stored with the name ‘delta’.\n","\n","The layers of the network are iterated in reverse order, starting at the output and working backwards. This ensures that the neurons in the output layer have ‘delta’ values calculated first that neurons in the hidden layer can use in the subsequent iteration.\n","\n","The error signal for neurons in the hidden layer is accumulated from neurons in the output layer where the hidden neuron number j is also the index of the neuron’s weight in the output layer neuron['weights'][j]."]},{"cell_type":"markdown","metadata":{"id":"NInzyHFnENJM"},"source":["# Exercise 9"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nb6jybJhEOfJ","executionInfo":{"status":"ok","timestamp":1612945845272,"user_tz":-60,"elapsed":625,"user":{"displayName":"César Leblanc","photoUrl":"","userId":"05479109431525987865"}},"outputId":"5e9380ae-84a0-4809-e13a-380f8f4a22f6"},"source":["seed(1)\r\n","\r\n","# test backpropagation of error\r\n","network = initialize_network(2, 1, 2)\r\n","expected = [0, 1]\r\n","forward_propagate(network, expected)\r\n","backward_propagate_error(network, expected)\r\n","for layer in network:\r\n","\tprint(layer)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'output': 0.8335790831681538, 'delta': -0.0020469977629036075}]\n","[{'weights': [0.2550690257394217, 0.49543508709194095], 'output': 0.6699713080575971, 'delta': -0.1481371914045779}, {'weights': [0.4494910647887381, 0.651592972722763], 'output': 0.7361939293022448, 'delta': 0.05123441744823936}]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QxRyraffwsgu"},"source":["Running the example prints the network after the backpropagation of error is complete. Error values are calculated and stored in the neurons for the output layer and the hidden layer."]}]}